{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understanding-software",
   "metadata": {
    "id": "understanding-software"
   },
   "source": [
    "# Practice HW 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-freeze",
   "metadata": {
    "id": "declared-freeze"
   },
   "source": [
    "In this homework, it is better to use the __wandb__ service for experiment monitoring. You can find the documentation [here](https://docs.wandb.ai/quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-fisher",
   "metadata": {
    "id": "certain-fisher"
   },
   "source": [
    "## 1. Personalized PageRank (5 points)\n",
    "\n",
    "Implement a personalized PageRank algorithm based on the seminar code with an option to return to a single point (the index provided in `keep_updating_until_convergence(...)`), or a topic-related version with the ability to return to a specific array of points (also provided to the same function). Think about the implementation, describe how you decided to modify the algorithm, and demonstrate its functionality on an artificial dataset (a bipartite graph can be used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-onion",
   "metadata": {
    "id": "headed-onion"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-carnival",
   "metadata": {
    "id": "enclosed-carnival"
   },
   "source": [
    "## Implementation of Graph Neural Network Layers for Node Classification (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-newman",
   "metadata": {
    "id": "lined-newman"
   },
   "source": [
    "In Semina , we implemented the **GraphSAGE** layer ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)). Here, you need to implement even more powerful layers: **GAT** ([Veličković et al. (2018)](https://arxiv.org/abs/1710.10903)) and **GCN** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)). Then, you should run the models to solve the node classification task on the CORA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-fleet",
   "metadata": {
    "id": "placed-fleet"
   },
   "outputs": [],
   "source": [
    "# !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
    "# !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
    "# !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-avenue",
   "metadata": {
    "id": "damaged-avenue"
   },
   "source": [
    "Below is a general GNN module where you can add any implemented GNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-pakistan",
   "metadata": {
    "id": "incoming-pakistan"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "\n",
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "        conv_model = self.build_conv_model(args.model_type)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers-1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout), \n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "\n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == 'GraphSage':\n",
    "            return GraphSage\n",
    "        elif model_type == 'GAT':\n",
    "            # When applying GAT with num heads > 1, you need to modify the \n",
    "            # input and output dimension of the conv layers (self.convs),\n",
    "            # to ensure that the input dim of the next layer is num heads\n",
    "            # multiplied by the output dim of the previous layer.\n",
    "            # HINT: In case you want to play with multiheads, you need to change the for-loop that builds up self.convs to be\n",
    "            # self.convs.append(conv_model(hidden_dim * num_heads, hidden_dim)), \n",
    "            # and also the first nn.Linear(hidden_dim * num_heads, hidden_dim) in post-message-passing.\n",
    "            return GAT\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "          \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout,training=self.training)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        if self.emb == True:\n",
    "            return x\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-soccer",
   "metadata": {
    "id": "returning-soccer"
   },
   "source": [
    "### GCN (4 points)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-philosophy",
   "metadata": {
    "id": "athletic-philosophy"
   },
   "source": [
    "The GCN layer is mathematically defined as  \n",
    "\n",
    "$$\\mathbf{x}_i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot \\left( \\mathbf{W}^{\\top} \\cdot \\mathbf{x}_j^{(k-1)} \\right) + \\mathbf{b}$$\n",
    "\n",
    "where the features of neighboring nodes are first transformed by the weight matrix \\( W \\), normalized by their degree, and summed. Finally, we apply the bias vector \\( b \\) to the aggregated result. This formula can be divided into the following steps:\n",
    "\n",
    "1. Add self-loops to the adjacency matrix (you can use the function `add_self_loops` from `torch_geometric`).\n",
    "2. Linear transformation of the node feature matrix.\n",
    "3. Compute normalization coefficients.\n",
    "4. Normalize the node features.\n",
    "5. Sum the features of neighboring nodes (aggregation “add”).\n",
    "6. Apply the bias vector.\n",
    "\n",
    "Steps 1-3 are usually computed before message passing. Steps 4-5 can be easily performed using the base class `MessagePassing`. Below, you need to implement the methods *message* and *forward* based on message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-harris",
   "metadata": {
    "id": "fatty-harris"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # TODO: Your code here! \n",
    "        pass\n",
    "\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # TODO: Your code here! \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-height",
   "metadata": {
    "id": "institutional-height"
   },
   "source": [
    "### GAT (6 points)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-ancient",
   "metadata": {
    "id": "transparent-ancient"
   },
   "source": [
    "Below, you need to implement the methods *message*, *forward*, and *aggregate* for the **Graph Attention Network (GAT)** layer based on message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-sunday",
   "metadata": {
    "id": "stainless-sunday"
   },
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, heads = 2,\n",
    "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
    "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin_l = None\n",
    "        self.lin_r = None\n",
    "        self.att_l = None\n",
    "        self.att_r = None\n",
    "\n",
    "        # TODO: Your code here!\n",
    "        \n",
    "        # Define the layers needed for the message functions\n",
    "        # self.lin_l is the linear transformation that you apply to embeddings \n",
    "        # before message passing\n",
    "\n",
    "        self.lin_r = self.lin_l\n",
    "\n",
    "        # TODO: Your code here! \n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
    "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
    "        nn.init.xavier_uniform_(self.att_l)\n",
    "        nn.init.xavier_uniform_(self.att_r)\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        H, C = self.heads, self.out_channels\n",
    "        # TODO: Your code here! \n",
    "        pass\n",
    "\n",
    "\n",
    "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
    "        # TODO: Your code here! \n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "        # TODO: Your code here! \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-cattle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T21:07:57.081849Z",
     "start_time": "2022-11-02T21:07:57.075973Z"
    },
    "id": "nuclear-cattle"
   },
   "source": [
    "Below is the code for testing GNN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-calvin",
   "metadata": {
    "id": "western-calvin"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler is None:\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-curve",
   "metadata": {
    "id": "charitable-curve"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(dataset, args):\n",
    "    print(\"Node task. test set size:\", np.sum(dataset[0]['test_mask'].numpy()))\n",
    "    print()\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes, \n",
    "                     args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            pred = pred[batch.train_mask]\n",
    "            label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_acc = test(test_loader, model)\n",
    "            test_accs.append(test_acc)\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            test_accs.append(test_accs[-1])\n",
    "    \n",
    "    return test_accs, losses, best_model, best_acc, test_loader\n",
    "\n",
    "def test(loader, test_model, is_validation=False, save_model_preds=False, model_type=None):\n",
    "    test_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    # Note that Cora is only one graph!\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            pred = test_model(data).max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        mask = data.val_mask if is_validation else data.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = label[mask]\n",
    "\n",
    "        if save_model_preds:\n",
    "            print (\"Saving Model Predictions for Model Type\", model_type)\n",
    "\n",
    "            data = {}\n",
    "            data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "            data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "    total = 0\n",
    "    for data in loader.dataset:\n",
    "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
    "\n",
    "    return correct / total\n",
    "  \n",
    "class ObjectView(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-representation",
   "metadata": {
    "id": "successful-representation"
   },
   "outputs": [],
   "source": [
    "args = ObjectView({\n",
    "    'model_type': 'GraphSage',\n",
    "    'dataset': 'cora',\n",
    "    'num_layers': 2,\n",
    "    'heads': 1,\n",
    "    'batch_size': 32,\n",
    "    'hidden_dim': 32,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 500,\n",
    "    'opt': 'adam',\n",
    "    'opt_scheduler': None,\n",
    "    'opt_restart': 0,\n",
    "    'weight_decay': 5e-3,\n",
    "    'lr': 0.01\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-hobby",
   "metadata": {
    "id": "novel-hobby"
   },
   "source": [
    "Set up the arguments and run the `train` and `test` functions on different layers (*GraphSAGE* from the seminar, *GCN*, *GAT*) on the Cora dataset and any other dataset of your choice available in the `torch_geometric` library. Compare the obtained accuracy across different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-measure",
   "metadata": {
    "id": "regulation-measure"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "charitable-ranch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T21:08:21.821439Z",
     "start_time": "2022-11-02T21:08:21.816723Z"
    },
    "id": "charitable-ranch"
   },
   "source": [
    "Also, experiment with the network parameters and, based on the experiments, write the conclusions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-drunk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:55:14.854665Z",
     "start_time": "2022-11-02T20:55:14.816633Z"
    },
    "id": "suited-drunk"
   },
   "source": [
    "**Report on Experiments:**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-damage",
   "metadata": {
    "id": "vietnamese-damage"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "useful-scoop",
   "metadata": {
    "id": "useful-scoop"
   },
   "source": [
    "## Bonus (5 points)  \n",
    "* Attach a link to the experiment report from **Weights & Biases (wandb)** (1 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-restaurant",
   "metadata": {
    "id": "advisory-restaurant"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
